{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zXnosfjbYg1Qn9ac7mTikFljWaGmzErf","timestamp":1754695414479}],"gpuType":"A100","machine_shape":"hm","mount_file_id":"1oyXtH7HRkzjqZL9jzPm92xPAZMAXnG5L","authorship_tag":"ABX9TyM/jrAez1y2UUo1M0ApEwKT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHCnfJvPyOkg","executionInfo":{"status":"ok","timestamp":1754854204386,"user_tz":240,"elapsed":137,"user":{"displayName":"Scott Josephson","userId":"14251408131454722197"}},"outputId":"50a11e5e-bffb-419c-8fe0-ccc4d0b6328a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[info] Multiprocessing start method set to 'spawn'.\n"]}],"source":["# sep_gpt_v0.py\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchaudio\n","import csv\n","import os\n","from torchaudio.transforms import MelSpectrogram\n","from typing import List, Dict, Tuple, Optional\n","\n","\n","# Set the multiprocessing start method to 'spawn' for CUDA compatibility\n","# This needs to be called once at the beginning of your script/notebook\n","# and before any multiprocessing (like DataLoader with num_workers > 0) is started.\n","try:\n","    torch.multiprocessing.set_start_method('spawn', force=True)\n","    print(\"[info] Multiprocessing start method set to 'spawn'.\")\n","except RuntimeError as e:\n","    print(f\"[warn] Could not set multiprocessing start method: {e}. It might already be set.\")\n","\n","# =========================\n","# Config (minimal recipe)\n","# =========================\n","class Cfg:\n","    sr = 44100\n","    # === STFT (Option A) ===\n","    # Smaller STFT to reduce token count so target fits under max_seq_len\n","    n_fft = 256\n","    hop = 256             # no overlap in STFT itself (we do overlap at windowing stage)\n","    win = 256\n","    pad = 0\n","    center = True\n","    power = 1.0           # magnitude (not power) for STFT\n","\n","    # === Mel reducer ===\n","    use_mel = True\n","    n_mels = 64\n","    mel_fmin = 0.0\n","    mel_fmax = None  # defaults to sr/2 if None\n","\n","    # Tokenization range on log10 magnitude (clipped)\n","    logmag_min_db = -12.0  # ~ -12 dB relative floor after per-window normalize\n","    logmag_max_db = 2.0\n","    n_bits = 8             # 8-bit uniform quant\n","\n","    # Model\n","    d_model = 512\n","    n_heads = 8\n","    n_layers = 8\n","    ff_mult = 4\n","    dropout = 0.1\n","\n","    # Windows\n","    seconds = 0.1   # short inference window to keep sequence under max_seq_len\n","    max_seq_len = 4096\n","\n","    # Stems (dataset uses subfolders: vocals/, drums/, bass/, guitar/)\n","    stems = [\"VOCALS\", \"DRUMS\", \"BASS\", \"GUITAR\"]\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","cfg = Cfg()\n","\n","# =========================\n","# Helper: STFT / iSTFT\n","# =========================\n","def stft_mag_phase(wav: torch.Tensor):\n","    \"\"\"\n","    wav: (T,) mono tensor, float32 [-1,1]\n","    returns: mag (frames, freq), phase (frames, freq)\n","    \"\"\"\n","    # Use a rectangular window (ones) with hop=win for valid overlap-add\n","    # This is a workaround for \"window overlap add min: 1\" error with Hann window\n","    window = torch.ones(cfg.win, device=wav.device) # Changed from torch.hann_window\n","    stft = torch.stft(\n","        wav,\n","        n_fft=cfg.n_fft,\n","        hop_length=cfg.hop,\n","        win_length=cfg.win,\n","        window=window,\n","        center=cfg.center,\n","        return_complex=True,\n","        pad_mode=\"reflect\",\n","    )  # (freq, frames)\n","    spec = stft.transpose(0, 1)  # (frames, freq)\n","    mag = spec.abs()\n","    phase = torch.angle(spec)\n","    return mag, phase\n","\n","def istft_from_mag_phase(mag: torch.Tensor, phase: torch.Tensor, length: int):\n","    \"\"\"\n","    mag/phase: (frames, freq)\n","    length: samples to trim/pad to\n","    \"\"\"\n","    spec = (mag * torch.exp(1j * phase)).transpose(0, 1)  # (freq, frames)\n","    # Use a rectangular window (ones) consistent with stft_mag_phase\n","    window = torch.ones(cfg.win, device=mag.device) # Changed from torch.hann_window\n","    wav = torch.istft(\n","        spec,\n","        n_fft=cfg.n_fft,\n","        hop_length=cfg.hop,\n","        win_length=cfg.win,\n","        window=window,\n","        center=cfg.center,\n","        length=length,\n","    )\n","    return wav\n","\n","# =========================\n","# Quantization (log-mag)\n","# =========================\n","def to_logmag(mag: torch.Tensor, eps=1e-8):\n","    # Per-window normalize: scale by max to stabilize, then log10\n","    m = torch.clamp(mag / (mag.max() + eps), min=eps)\n","    logm = torch.log10(m)\n","    # clamp to configured range\n","    logm = torch.clamp(\n","        logm,\n","        math.log10(10 ** (cfg.logmag_min_db / 20)),\n","        math.log10(10 ** (cfg.logmag_max_db / 20)),\n","    )\n","    return logm\n","\n","def quantize_logmag(logm: torch.Tensor):\n","    \"\"\"\n","    logm: (frames, freq_or_mel)\n","    returns: tokens (frames*feat,)\n","    \"\"\"\n","    lo = math.log10(10 ** (cfg.logmag_min_db / 20))\n","    hi = math.log10(10 ** (cfg.logmag_max_db / 20))\n","    qlevels = (1 << cfg.n_bits)\n","    x = (logm - lo) / (hi - lo)  # [0,1]\n","    x = torch.clamp(x, 0, 1)\n","    q = torch.round(x * (qlevels - 1)).to(torch.long)  # [0..255]\n","    return q.view(-1)\n","\n","def dequantize_to_mag(tokens: torch.Tensor, shape):\n","    \"\"\"\n","    tokens: (frames*feat,)\n","    shape: (frames, feat)\n","    returns: magnitude-like features (frames, feat)\n","    \"\"\"\n","    lo = math.log10(10 ** (cfg.logmag_min_db / 20))\n","    hi = math.log10(10 ** (cfg.logmag_max_db / 20))\n","    qlevels = (1 << cfg.n_bits)\n","    x = tokens.float() / (qlevels - 1)\n","    logm = x * (hi - lo) + lo\n","    m = 10 ** logm  # invert log10 amplitude\n","    return m.view(*shape)\n","\n","# =========================\n","# Mel reducer (linear <-> mel), deterministic DSP (no NN)\n","# =========================\n","_mel_cache = {}\n","def _get_mel_mats(device):\n","    \"\"\"\n","    Returns (fb, pinv) where:\n","      fb:   (F, M) linear->mel filterbank\n","      pinv: (M, F) pseudo-inverse for mel->linear\n","    Cached per (n_fft, sr, n_mels, device).\n","    \"\"\"\n","    key = (cfg.n_fft, cfg.sr, cfg.n_mels, device)\n","    if key in _mel_cache:\n","        return _mel_cache[key]\n","\n","    n_freqs = cfg.n_fft // 2 + 1\n","    n_mels = cfg.n_mels\n","\n","    # Use MelSpectrogram to get the filterbank\n","    mel_spectrogram = MelSpectrogram(\n","        sample_rate=cfg.sr,\n","        n_fft=cfg.n_fft,\n","        n_mels=cfg.n_mels,\n","        f_min=cfg.mel_fmin,\n","        f_max=cfg.mel_fmax,\n","        hop_length=cfg.hop,\n","        win_length=cfg.win,\n","        center=cfg.center,\n","        pad=cfg.pad,\n","        power=cfg.power, # Note: MelSpectrogram typically uses power=2.0, but our STFT is power=1.0.\n","                         # We'll use the filterbank from MelSpectrogram and apply it to our magnitude STFT.\n","        norm='slaney' if hasattr(MelSpectrogram, '_get_slaney_mel_scale') else None, # Use Slaney norm if available\n","        mel_scale=\"htk\"\n","    ).to(device)\n","\n","    # Get the filterbank matrix from the MelSpectrogram object\n","    # The standard shape is (n_mels, n_freqs) = (M, F)\n","    # Accessing it directly via mel_scale.fb is typical in recent torchaudio versions\n","    try:\n","        fb_raw = mel_spectrogram.mel_scale.fb\n","    except AttributeError:\n","         raise RuntimeError(\"Could not access mel filterbank from MelSpectrogram object. Ensure torchaudio version is compatible.\")\n","\n","\n","    # Ensure the filterbank has the shape (F, M) = (n_freqs, n_mels) for linear_to_mel\n","    if fb_raw.shape == (n_mels, n_freqs):\n","        fb = fb_raw.transpose(0, 1)  # Transpose from (M, F) to (F, M)\n","    elif fb_raw.shape == (n_freqs, n_mels):\n","        fb = fb_raw  # It's already in (F, M) shape\n","    else:\n","        raise RuntimeError(f\"Unexpected mel filterbank shape: {fb_raw.shape}. Expected ({n_mels}, {n_freqs}) or ({n_freqs}, {n_mels})\")\n","\n","\n","    # Compute the pseudo-inverse for mel_to_linear\n","    pinv = torch.pinverse(fb).to(device)  # pinv will be (M, F) if fb is (F, M)\n","\n","    _mel_cache[key] = (fb, pinv)\n","    return _mel_cache[key]\n","\n","def linear_to_mel(mag_linear: torch.Tensor):\n","    \"\"\"\n","    mag_linear: (frames, F)\n","    returns mel magnitudes: (frames, M)\n","    \"\"\"\n","    fb, _ = _get_mel_mats(mag_linear.device)\n","    # The error occurs here: mag_linear (frames, F) @ fb (F, M)\n","    # Error message showed (18x129) @ (64x129)\n","    # This implies fb is (64, 129). It should be (129, 64).\n","    # The logic in _get_mel_mats should now ensure fb is (129, 64)\n","    # so this multiplication becomes (18x129) @ (129x64) resulting in (18x64)\n","    return mag_linear @ fb\n","\n","def mel_to_linear(mag_mel: torch.Tensor):\n","    _, pinv = _get_mel_mats(mag_mel.device)\n","    # pinv should be (M, F) = (64, 129)\n","    # mag_mel is (frames, M) = (frames, 64)\n","    # Multiplication: (frames, 64) @ (64, 129) -> (frames, 129)\n","    return torch.clamp(mag_mel @ pinv, min=0.0)\n","\n","# =========================\n","# Token vocab & packing\n","# =========================\n","class Vocab:\n","    # quantized bins 0..255\n","    # special/control tokens appended after\n","    PAD = 256\n","    BOS = 257\n","    EOS = 258\n","    SEP = 259\n","    MIX = 260\n","    STEM_BASE = 300  # STEM tokens at STEM_BASE + idx\n","\n","    def __init__(self):\n","        self.num_bins = 1 << cfg.n_bits\n","        self.stem_to_id = {name: self.STEM_BASE + i for i, name in enumerate(cfg.stems)}\n","        self.id_to_stem = {v: k for k, v in self.stem_to_id.items()}\n","        self.vocab_size = self.STEM_BASE + len(cfg.stems)\n","\n","vocab = Vocab()\n","\n","def pack_sequence(mix_tokens: torch.Tensor, stem_name: str, target_tokens: torch.Tensor):\n","    \"\"\"\n","    Create LM sequence:\n","    [BOS, MIX, mix..., SEP, STEM(token), target..., EOS]\n","    Returns tokens (L,), loss_mask (L,) where mask=1 for target positions only.\n","    \"\"\"\n","    stem_tok = torch.tensor([vocab.stem_to_id[stem_name]], device=mix_tokens.device, dtype=torch.long)\n","    seq = torch.cat([\n","        torch.tensor([vocab.BOS, vocab.MIX], device=mix_tokens.device),\n","        mix_tokens,\n","        torch.tensor([vocab.SEP], device=mix_tokens.device),\n","        stem_tok,\n","        target_tokens,\n","        torch.tensor([vocab.EOS], device=mix_tokens.device),\n","    ])\n","    # Loss mask: predict from position after STEM token inclusive\n","    start = (2 + mix_tokens.numel() + 1 + 1)  # after BOS,MIX + mix + SEP + STEM\n","    loss_mask = torch.zeros_like(seq, dtype=torch.bool)\n","    # positions where target tokens + EOS reside\n","    loss_mask[start:] = True\n","    return seq, loss_mask\n","\n","def split_mix_target_from_seq(seq: torch.Tensor):\n","    # helper for debugging; not needed in training loop\n","    pass\n","\n","# =========================\n","# GPT-style decoder-only model\n","# =========================\n","class GPTBlock(nn.Module):\n","    def __init__(self, d_model, n_heads, ff_mult, dropout):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(d_model)\n","        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n","        self.ln2 = nn.LayerNorm(d_model)\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, ff_mult * d_model),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(ff_mult * d_model, d_model),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x, attn_mask):\n","        h = self.ln1(x)\n","        # attn_mask may be (L,L) or (B*n_heads, L, L). We pass (B*n_heads, L, L)\n","        y, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n","        x = x + y\n","        h = self.ln2(x)\n","        x = x + self.ff(h)\n","        return x\n","\n","class GPT(nn.Module):\n","    def __init__(self, vocab_size, d_model, n_layers, n_heads, ff_mult, dropout):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        # Use cfg.max_seq_len for positional embedding\n","        self.pos = nn.Parameter(torch.zeros(1, cfg.max_seq_len, d_model))\n","        self.blocks = nn.ModuleList([\n","            GPTBlock(d_model, n_heads, ff_mult, dropout) for _ in range(n_layers)\n","        ])\n","        self.ln = nn.LayerNorm(d_model)\n","        self.out = nn.Linear(d_model, vocab_size)\n","        self.n_heads = n_heads  # Store n_heads for mask creation\n","\n","    def forward(self, tokens, loss_mask=None):\n","        \"\"\"\n","        tokens: (B, L) long\n","        loss_mask: (B, L) bool (optional)\n","        returns: logits (B, L, V), loss (scalar) if mask provided\n","        \"\"\"\n","        B, L = tokens.shape\n","        max_pos = self.pos.size(1)\n","        if L > max_pos:\n","            tokens = tokens[:, :max_pos]\n","            if loss_mask is not None:\n","                loss_mask = loss_mask[:, :max_pos]\n","            L = max_pos\n","        x = self.embed(tokens) + self.pos[:, :L, :]\n","        # causal mask: no lookahead\n","        causal = torch.triu(torch.ones(L, L, device=tokens.device, dtype=torch.bool), diagonal=1)\n","        # Repeat causal mask for each batch*head (supported by MultiheadAttention)\n","        causal = causal.unsqueeze(0).repeat(B * self.n_heads, 1, 1)\n","\n","        for blk in self.blocks:\n","            x = blk(x, causal)\n","        x = self.ln(x)\n","        logits = self.out(x)\n","        loss = None\n","        if loss_mask is not None:\n","            # shift targets by one (next token prediction)\n","            logits_shift = logits[:, :-1, :].contiguous()\n","            targets = tokens[:, 1:].contiguous()\n","            mask = loss_mask[:, 1:].contiguous()\n","            if mask.view(-1).sum() > 0:\n","                loss = F.cross_entropy(\n","                    logits_shift.view(-1, logits.size(-1))[mask.view(-1)],\n","                    targets.view(-1)[mask.view(-1)]\n","                )\n","            else:\n","                loss = torch.tensor(0.0, device=logits.device)\n","        return logits, loss\n","\n","# =========================\n","# Data demo utilities\n","# =========================\n","def pad_or_trim(wav, length):\n","    T = wav.numel()\n","    if T >= length:\n","        return wav[:length]\n","    out = torch.zeros(length, device=wav.device)\n","    out[:T] = wav\n","    return out\n","\n","def mono(wav):\n","    if wav.dim() == 2:\n","        return wav.mean(0)\n","    return wav\n","\n","def load_wav(path, target_sr):\n","    wav, sr = torchaudio.load(path)\n","    wav = mono(wav)\n","    if sr != target_sr:\n","        wav = torchaudio.functional.resample(wav, sr, target_sr)\n","    wav = wav.clamp(-1, 1)\n","    return wav\n","\n","# =========================\n","# End-to-end: one batch example\n","# =========================\n","def make_batch(mix_wav, stem_wav, stem_name: str):\n","    \"\"\"\n","    Build a (1, L) token batch and loss mask for one window.\n","    \"\"\"\n","    device = cfg.device\n","    # STFTs\n","    mix_mag, mix_phase = stft_mag_phase(mix_wav)\n","    stem_mag, _ = stft_mag_phase(stem_wav)\n","\n","    # Tokenize in MEL space (64 bands) if enabled\n","    if cfg.use_mel:\n","        mix_feat = linear_to_mel(mix_mag)\n","        stem_feat = linear_to_mel(stem_mag)\n","    else:\n","        mix_feat, stem_feat = mix_mag, stem_mag\n","    mix_log = to_logmag(mix_feat)\n","    stem_log = to_logmag(stem_feat)\n","    mix_tok = quantize_logmag(mix_log)\n","    stem_tok = quantize_logmag(stem_log)\n","\n","    # Pack\n","    seq, mask = pack_sequence(mix_tok, stem_name, stem_tok)\n","\n","    # Trim to max_seq_len\n","    if seq.numel() > cfg.max_seq_len:\n","        seq = seq[:cfg.max_seq_len]\n","        mask = mask[:cfg.max_seq_len]\n","\n","    feat_shape = mix_feat.shape  # (frames, M or F) used to reshape target tokens\n","    return seq.unsqueeze(0).to(device), mask.unsqueeze(0).to(device), mix_phase, feat_shape\n","\n","@torch.no_grad()\n","def greedy_decode(model, mix_tokens, stem_name: str, max_new=None):\n","    \"\"\"\n","    mix_tokens: (Lmix,) long\n","    Returns full sequence including prefix+generated tokens.\n","    \"\"\"\n","    device = cfg.device\n","    stem_tok = torch.tensor([vocab.stem_to_id[stem_name]], device=device, dtype=torch.long)\n","    seq = torch.cat([\n","        torch.tensor([vocab.BOS, vocab.MIX], device=device),\n","        mix_tokens.to(device),\n","        torch.tensor([vocab.SEP], device=device),\n","        stem_tok\n","    ])\n","    seq = seq.unsqueeze(0)  # (1, L)\n","    # generate until EOS or max_new\n","    max_new = max_new or (mix_tokens.numel() + 1024)  # cap\n","    # Cap generation length at max_seq_len\n","    max_new = min(max_new, cfg.max_seq_len - seq.size(1))\n","    for _ in range(max_new):\n","        logits, _ = model(seq)\n","        next_logits = logits[:, -1, :]\n","        next_tok = torch.argmax(next_logits, dim=-1)  # shape: (1,)\n","        seq = torch.cat([seq, next_tok.unsqueeze(1)], dim=1)\n","        if int(next_tok.item()) == vocab.EOS:\n","            break\n","    return seq.squeeze(0)\n","\n","def reconstruct_from_sequence(seq, mix_phase, feat_shape, target_len):\n","    \"\"\"\n","    Extract predicted target tokens from seq and reconstruct WAV via mixture phase.\n","    \"\"\"\n","    # Locate SEP index\n","    sep_pos = (seq == vocab.SEP).nonzero(as_tuple=True)[0]\n","    if sep_pos.numel() == 0:\n","        print(\"Warning: SEP token not found in sequence. Reconstruction may be incomplete.\")\n","        # If SEP is not found, assume the entire sequence after the prefix is target tokens\n","        # (2 for BOS, MIX + mix_tokens length)\n","        prefix_len = 2 + (feat_shape[0] * feat_shape[1])  # Approximate mix token length from feature shape\n","        start = prefix_len + 1 + 1  # after BOS, MIX, mix_tokens, SEP, STEM\n","        start = min(start, seq.numel())  # Ensure start is within bounds\n","        predicted_tokens_segment = seq[start:]\n","    else:\n","        sep_idx = sep_pos[-1].item()\n","        start = sep_idx + 2  # skip SEP and STEM token\n","        # Collect predicted tokens until EOS or end of sequence\n","        eos_pos = (seq[start:] == vocab.EOS).nonzero(as_tuple=True)[0]\n","        if eos_pos.numel() > 0:\n","            end = start + int(eos_pos[0].item())\n","        else:\n","            end = len(seq)\n","        predicted_tokens_segment = seq[start:end]\n","\n","    # Determine the expected length of the target token sequence (mel or linear feature length)\n","    expected_len = feat_shape[0] * feat_shape[1]\n","\n","    # Explicitly pad or trim the predicted tokens to the expected length\n","    if predicted_tokens_segment.numel() < expected_len:\n","        pad_len = expected_len - predicted_tokens_segment.numel()\n","        # Pad with a valid quantized bin (e.g., 0 for lowest magnitude)\n","        tgt_tokens_padded = torch.cat(\n","            [predicted_tokens_segment, torch.full((pad_len,), 0, device=predicted_tokens_segment.device, dtype=torch.long)]\n","        )\n","    else:\n","        tgt_tokens_padded = predicted_tokens_segment[:expected_len]  # Trim if longer\n","\n","    # Dequantize to feature magnitude (mel or linear)\n","    pred_feat_mag = dequantize_to_mag(tgt_tokens_padded.detach(), feat_shape).to(mix_phase.device)\n","    # If using mel, convert back to linear frequency bins before iSTFT\n","    if cfg.use_mel:\n","        pred_mag_linear = mel_to_linear(pred_feat_mag)\n","    else:\n","        pred_mag_linear = pred_feat_mag\n","\n","    # Use mixture phase for reconstruction\n","    wav = istft_from_mag_phase(pred_mag_linear, mix_phase, target_len)\n","    return wav.clamp(-1, 1)\n","\n","def demo_step(model, optimizer, mix_wav, stem_wav, stem_name=\"VOCALS\"):\n","    \"\"\"\n","    Runs one supervised step and a greedy decode.\n","    \"\"\"\n","    # Build batch\n","    seq, mask, mix_phase, feat_shape = make_batch(mix_wav, stem_wav, stem_name)\n","    # Visibility into how many target tokens are being trained this step\n","    mask_tokens = int(mask.sum().item())\n","    print(f\"[debug] loss_mask target tokens this step: {mask_tokens}\")\n","    # assert mask_tokens > 0, \"Loss mask is empty—target tokens were entirely truncated.\"\n","\n","    model.train()\n","    logits, loss = model(seq, loss_mask=mask)\n","    optimizer.zero_grad()\n","    if loss is not None and loss.requires_grad:\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","    # Greedy decode (eval)\n","    model.eval()\n","    # The mix_tokens slice corresponds to tokens between MIX and SEP\n","    sep_pos = (seq[0] == vocab.SEP).nonzero(as_tuple=True)[0]\n","    if sep_pos.numel() > 0:\n","        mix_tok_end = sep_pos[0].item()\n","    else:\n","        mix_tok_end = 2 + (feat_shape[0] * feat_shape[1])\n","        mix_tok_end = min(mix_tok_end, seq.size(1))\n","    mix_tok = seq[0, 2:mix_tok_end]\n","\n","    with torch.no_grad():\n","        full = greedy_decode(model, mix_tok, stem_name=stem_name)\n","    # Reconstruct predicted stem\n","    target_len = mix_wav.numel()\n","    pred_wav = reconstruct_from_sequence(full, mix_phase, feat_shape, target_len)\n","    return float(loss.item()) if loss is not None else float('nan'), pred_wav\n","\n","# =========================\n","# Inference: windowed separation with overlap–add\n","# =========================\n","def build_model():\n","    return GPT(\n","        vocab_size=vocab.vocab_size,\n","        d_model=cfg.d_model,\n","        n_layers=cfg.n_layers,\n","        n_heads=cfg.n_heads,\n","        ff_mult=cfg.ff_mult,\n","        dropout=cfg.dropout\n","    ).to(cfg.device)\n","\n","def load_checkpoint(model: nn.Module, ckpt_path: str):\n","    if ckpt_path is None:\n","        print(\"[warn] No checkpoint provided. Using randomly initialized weights (predictions will sound like noise).\")\n","        return\n","    state = torch.load(ckpt_path, map_location=cfg.device)\n","    if isinstance(state, dict) and \"state_dict\" in state:\n","        state = state[\"state_dict\"]\n","    model.load_state_dict(state, strict=False)\n","    print(f\"[info] Loaded checkpoint from: {ckpt_path}\")\n","\n","def separate_stems_from_mix(model: nn.Module, mix_wav: torch.Tensor, stems=None, overlap: float = 0.5):\n","    \"\"\"\n","    mix_wav: (T,) mono on cfg.device\n","    stems: list of stem names to generate (defaults to cfg.stems)\n","    overlap: fraction in [0,1). 0.5 -> 50% overlap.\n","    returns dict {stem_name: (T,) tensor}\n","    \"\"\"\n","    stems = stems or cfg.stems\n","    device = cfg.device\n","    T_total = mix_wav.numel()\n","    T_win = int(cfg.seconds * cfg.sr)\n","    assert T_win > 0, \"Window size (seconds * sr) must be > 0\"\n","    hop_win = max(1, int(T_win * (1.0 - overlap)))\n","    print(f\"[info] Separation with window={T_win} samples ({cfg.seconds:.3f}s), hop={hop_win} samples, overlap={overlap:.2f}\")\n","\n","    # Hann for overlap–add (fade)\n","    fade = torch.hann_window(T_win, periodic=False, device=device)\n","\n","    # Outputs and weight accumulator\n","    outs = {stem: torch.zeros(T_total, device=device) for stem in stems}\n","    weight = torch.zeros(T_total, device=device)\n","\n","    pos = 0\n","    while pos < T_total:\n","        end = min(pos + T_win, T_total)\n","        # Extract and pad this chunk to T_win\n","        chunk = pad_or_trim(mix_wav[pos:end], T_win)\n","        # STFT -> (optional) MEL -> tokens\n","        mix_mag, mix_phase = stft_mag_phase(chunk)\n","        if cfg.use_mel:\n","            mix_feat = linear_to_mel(mix_mag)\n","            feat_shape = mix_feat.shape\n","        else:\n","            mix_feat = mix_mag\n","            feat_shape = mix_mag.shape\n","        mix_log = to_logmag(mix_feat)\n","        mix_tok = quantize_logmag(mix_log)\n","\n","        # Decode each requested stem\n","        for stem in stems:\n","            with torch.no_grad():\n","                full_seq = greedy_decode(model, mix_tok, stem_name=stem)\n","                pred_chunk = reconstruct_from_sequence(full_seq, mix_phase, feat_shape, T_win)  # (T_win,)\n","\n","            # Overlap–add with fade\n","            L = end - pos  # actual (possibly shorter) region\n","            outs[stem][pos:end] += (pred_chunk[:L] * fade[:L])\n","        weight[pos:end] += fade[:L]\n","\n","        pos += hop_win\n","\n","    # Normalize by accumulated weights to avoid gain on overlaps\n","    eps = 1e-8\n","    for stem in stems:\n","        outs[stem] = (outs[stem] / (weight + eps)).clamp(-1, 1)\n","    return outs\n","\n","def separate_file(model: nn.Module, mix_path: str, out_dir: str = \".\", stems: list[str] | None = None,\n","                   overlap: float = 0.5, ckpt: str | None = None):\n","    os.makedirs(out_dir, exist_ok=True)\n","    wav = load_wav(mix_path, cfg.sr).to(cfg.device)\n","    wav = mono(wav)\n","    outs = separate_stems_from_mix(model, wav, stems=stems, overlap=overlap)\n","    for stem, audio in outs.items():\n","        out_path = os.path.join(out_dir, f\"{stem.lower()}.wav\")\n","        torchaudio.save(out_path, audio.unsqueeze(0).cpu(), cfg.sr)\n","        print(f\"[write] {out_path}\")\n","\n","# =========================\n","# Dataset + Training\n","# =========================\n","def _default_stem_files() -> Dict[str, str]:\n","    \"\"\"\n","    Map our logical stem names to **subdirectory** names in each track folder.\n","    For the provided layout we expect, per track GUID:\n","        <GUID>/vocals/<wav>\n","        <GUID>/drums/<wav>\n","        <GUID>/bass/<wav>\n","        <GUID>/guitar/<wav>\n","    NOTE: name values here are directory names, not filenames.\n","    \"\"\"\n","    return {\n","        \"VOCALS\": \"vocals\",\n","        \"DRUMS\":  \"drums\",\n","        \"BASS\":   \"bass\",\n","        \"GUITAR\": \"guitar\",\n","    }\n","\n","def _safe_lower(s: str) -> str:\n","    return s.lower() if isinstance(s, str) else s\n","\n","def _list_tracks(root: str) -> List[str]:\n","    tracks = [os.path.join(root, d) for d in os.listdir(root)\n","              if os.path.isdir(os.path.join(root, d))]\n","    tracks.sort()\n","    return tracks\n","\n","def split_dataset(root: str, val_fraction: float = 0.1, test_fraction: float = 0.1, seed: int = 42):\n","    \"\"\"Deterministic split into train/val/test lists of track directories.\"\"\"\n","    import random\n","    tracks = _list_tracks(root)\n","    rng = random.Random(seed)\n","    rng.shuffle(tracks)\n","    n = len(tracks); n_test = int(round(n*test_fraction)); n_val = int(round((n-n_test)*val_fraction))\n","    test_tracks = tracks[:n_test]\n","    val_tracks  = tracks[n_test:n_test+n_val]\n","    train_tracks = tracks[n_test+n_val:]\n","    print(f\"[split] total={n} → train={len(train_tracks)}, val={len(val_tracks)}, test={len(test_tracks)}\")\n","    return {\"train\": train_tracks, \"val\": val_tracks, \"test\": test_tracks}\n","\n","def _find_file_case_insensitive(folder: str, filename: str) -> Optional[str]:\n","    target = _safe_lower(filename)\n","    for f in os.listdir(folder):\n","        if _safe_lower(f) == target:\n","            return os.path.join(folder, f)\n","    return None\n","\n","def _find_stem_wav_from_subdir(track_dir: str, subdir_name: str) -> Optional[str]:\n","    \"\"\"\n","    Locate a WAV file under <track_dir>/<subdir_name>/\n","    Returns the first lexicographically sorted *.wav (case-insensitive), or None.\n","    \"\"\"\n","    if subdir_name is None:\n","        return None\n","    stem_dir = os.path.join(track_dir, subdir_name)\n","    if not os.path.isdir(stem_dir):\n","        return None\n","    candidates = [f for f in os.listdir(stem_dir) if f.lower().endswith(\".wav\")]\n","    candidates.sort()\n","    if not candidates:\n","        return None\n","    return os.path.join(stem_dir, candidates[0])\n","\n","class StemSeparationDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Expects a directory of track folders:\n","      root/<GUID>/\n","        vocals/<wav>\n","        drums/<wav>\n","        bass/<wav>\n","        guitar/<wav>\n","    Builds fixed-length windows (cfg.seconds) with a given hop (overlap in time domain).\n","    Each item returns (mix_window, stem_window, stem_name).\n","    \"\"\"\n","    def __init__(\n","        self,\n","        root: str,\n","        track_dirs: Optional[List[str]] = None,\n","        stems: Optional[List[str]] = None,\n","        window_seconds: float = None,\n","        overlap: float = 0.5,\n","        stem_files: Optional[Dict[str, str]] = None,\n","        mix_from_stems: bool = True,\n","    ):\n","        super().__init__()\n","        self.root = root\n","        self.track_dirs = track_dirs  # if provided, restrict dataset to these tracks\n","        self.stems = stems or cfg.stems\n","        self.window_seconds = cfg.seconds if window_seconds is None else float(window_seconds)\n","        self.overlap = float(overlap)\n","        assert 0.0 <= self.overlap < 1.0\n","        # Here 'stem_files' actually maps stem name -> subdirectory name\n","        self.stem_files = stem_files or _default_stem_files()\n","        self.mix_from_stems = bool(mix_from_stems)\n","        self.win_samples = int(self.window_seconds * cfg.sr)\n","        self.hop_samples = max(1, int(self.win_samples * (1.0 - self.overlap)))\n","\n","        # Index: list of (track_dir, stem_name, start_sample)\n","        self.index: List[Tuple[str, str, int]] = []\n","        self._build_index()\n","\n","    def _build_index(self):\n","        tracks = self.track_dirs if self.track_dirs is not None else _list_tracks(self.root)\n","        for track_dir in tracks:\n","            # Determine track duration from the longest available stem WAV in its subfolders\n","            max_frames = 0\n","            for s in self.stems:\n","                subdir = self.stem_files.get(s, None)\n","                sp = _find_stem_wav_from_subdir(track_dir, subdir)\n","                if sp is None:\n","                    continue\n","                try:\n","                    info = torchaudio.info(sp)\n","                    frames_at_target = int(round(info.num_frames * (cfg.sr / info.sample_rate)))\n","                    max_frames = max(max_frames, frames_at_target)\n","                except Exception as e:\n","                    print(f\"[warn] Could not read stem WAV in {track_dir}/{subdir}: {e}\")\n","                    continue\n","            if max_frames == 0:\n","                print(f\"[warn] No usable stem audio in {track_dir}; skipping.\")\n","                continue\n","            # Generate window start positions\n","            pos = 0\n","            while pos < max_frames:\n","                self.index.extend([(track_dir, stem, pos) for stem in self.stems])\n","                pos += self.hop_samples\n","\n","    def __len__(self):\n","        return len(self.index)\n","\n","    def _load_wav_window(self, path: str, start: int, length: int) -> torch.Tensor:\n","        # Load a small window; if backend doesn't support frame_offset efficiently, this will still work.\n","        wav, sr = torchaudio.load(path, frame_offset=max(0, start), num_frames=length)\n","        wav = mono(wav)\n","        if sr != cfg.sr:\n","            wav = torchaudio.functional.resample(wav, sr, cfg.sr)\n","        # If shorter than length due to end of file, pad\n","        wav = pad_or_trim(wav, length)\n","        return wav\n","\n","    def __getitem__(self, idx: int):\n","        track_dir, stem_name, start = self.index[idx]\n","        # Build mixture by summing stems (dataset layout has no mixture.wav)\n","        mix_win = torch.zeros(self.win_samples, dtype=torch.float32)\n","        for s in self.stems:\n","            subdir_s = self.stem_files.get(s, None)\n","            sp = _find_stem_wav_from_subdir(track_dir, subdir_s)\n","            if sp is None:\n","                continue\n","            mix_win = mix_win + self._load_wav_window(sp, start, self.win_samples)\n","        # conservative anti-clipping (per-window norm still applied later)\n","        maxabs = mix_win.abs().max().item()\n","        if maxabs > 1.0:\n","            mix_win = (mix_win / maxabs).clamp_(-1, 1)\n","        # Target stem\n","        stem_file = self.stem_files.get(stem_name, None)  # actually subdir name\n","        if stem_file is None:\n","            raise KeyError(f\"No filename mapping for stem '{stem_name}'\")\n","        stem_path = _find_stem_wav_from_subdir(track_dir, stem_file)\n","        if stem_path is None:\n","            # If stem missing, assume silence for that stem\n","            stem_win = torch.zeros_like(mix_win)\n","        else:\n","            stem_win = self._load_wav_window(stem_path, start, self.win_samples)\n","        # Return minimal metadata so we can tag window-level metrics\n","        return mix_win, stem_win, stem_name, track_dir, start\n","\n","def collate_pack(batch):\n","    \"\"\"\n","    batch: list of tuples (mix_win, stem_win, stem_name, track_dir, start)\n","    Returns:\n","      tokens: (B, L) long\n","      loss_mask: (B, L) bool\n","      targets: (B, T) float\n","      mix_phases: list of phase tensors per item\n","      feat_shapes: list of feature shapes per item\n","      wav_lens: list of target lengths per item\n","      stem_names: list of stem names\n","      track_names: list[str] base names for tracks\n","      starts: list[int] window start sample (at dataset SR)\n","    NOTE: With fixed cfg.seconds and fixed STFT/mel, all sequences L should match.\n","    \"\"\"\n","    tokens_list = []\n","    masks_list = []\n","    targets_list = []\n","    phases = []\n","    shapes = []\n","    wav_lens = []\n","    stems = []\n","    track_names = []\n","    starts = []\n","    for mix_win, stem_win, stem_name, track_dir, start in batch:\n","        mix_win = mix_win.to(cfg.device)\n","        stem_win = stem_win.to(cfg.device)\n","        targets_list.append(stem_win)\n","        seq, mask, mix_phase, feat_shape = make_batch(mix_win, stem_win, stem_name)\n","        tokens_list.append(seq[0])  # seq is (1, L)\n","        masks_list.append(mask[0])\n","        phases.append(mix_phase)\n","        shapes.append(feat_shape)\n","        wav_lens.append(mix_win.numel())\n","        stems.append(stem_name)\n","        track_names.append(os.path.basename(track_dir.rstrip(\"/\\\\\")))\n","        starts.append(int(start))\n","    tokens = torch.stack(tokens_list, dim=0)\n","    loss_mask = torch.stack(masks_list, dim=0)\n","    targets = torch.stack(targets_list, dim=0)\n","    return tokens, loss_mask, targets, phases, shapes, wav_lens, stems, track_names, starts\n","\n","@torch.no_grad()\n","def decode_batch(model, batch_tokens, phases, shapes, wav_lens, stems):\n","    \"\"\"\n","    Greedy-decodes each item in the batch for its requested stem.\n","    Returns list of waveforms (on device).\n","    \"\"\"\n","    outs = []\n","    B = batch_tokens.size(0)\n","    for i in range(B):\n","        seq = batch_tokens[i]\n","        # slice mixture tokens between MIX and SEP\n","        sep_pos = (seq == vocab.SEP).nonzero(as_tuple=True)[0]\n","        if sep_pos.numel() > 0:\n","            mix_tok_end = sep_pos[0].item()\n","        else:\n","            mix_tok_end = 2 + (shapes[i][0] * shapes[i][1])\n","            mix_tok_end = min(mix_tok_end, seq.size(0))\n","        mix_tok = seq[2:mix_tok_end]\n","        full = greedy_decode(model, mix_tok, stems[i])\n","        wav = reconstruct_from_sequence(full, phases[i], shapes[i], wav_lens[i])\n","        outs.append(wav)\n","    return outs\n","\n","def si_sdr(ref: torch.Tensor, est: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n","    \"\"\"\n","    Scale-invariant SDR in dB for 1D tensors on the same device.\n","    \"\"\"\n","    ref = ref - ref.mean()\n","    est = est - est.mean()\n","    ref_energy = torch.sum(ref**2) + eps\n","    scale = torch.sum(ref * est) / ref_energy\n","    proj = scale * ref\n","    noise = est - proj\n","    ratio = (torch.sum(proj**2) + eps) / (torch.sum(noise**2) + eps)\n","    return 10.0 * torch.log10(ratio + eps)\n","\n","@torch.no_grad()\n","def evaluate_sisdr(\n","    dataset_root: str,\n","    ckpt: str,\n","    batch_size: int = 2,\n","    overlap: float = 0.5,\n","    num_workers: int = 2,\n","    max_batches: int = 25,\n","    track_dirs: Optional[List[str]] = None,\n","    metrics_dir: Optional[str] = None,\n","):\n","    \"\"\"\n","    Window-level SI-SDR evaluation with decoding (greedy).\n","    Returns (avg_sisdr, per_stem_sisdr_dict). Also writes CSV if metrics_dir is provided\n","    (or defaults to ./metrics).\n","    Window-level evaluation with decoding (greedy).\n","      - Computes SI-SDR per window (decoded audio vs target).\n","      - Computes masked next-token cross-entropy (CE) per window on the same batch.\n","    Returns (avg_sisdr, per_stem_sisdr_dict). Also writes CSV (sisdr_windows.csv) to metrics_dir\n","    (or ./metrics if not provided).\n","    \"\"\"\n","    model = build_model()\n","    load_checkpoint(model, ckpt)\n","    ds = StemSeparationDataset(dataset_root, track_dirs=track_dirs, stems=cfg.stems,\n","                               window_seconds=cfg.seconds, overlap=overlap)\n","    if len(ds) == 0:\n","        raise RuntimeError(f\"No evaluation items found under {dataset_root}\")\n","    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n","                                     collate_fn=collate_pack, pin_memory=False)\n","    model.eval()\n","    total = 0.0; count = 0\n","    per_stem_sum: Dict[str, float] = {s: 0.0 for s in cfg.stems}\n","    per_stem_cnt: Dict[str, int] = {s: 0 for s in cfg.stems}\n","    # (track, start, stem, sisdr, ce_loss, num_masked_tokens)\n","    window_rows: List[Tuple[str,int,str,float,float,int]] = []\n","    it = 0\n","    for tokens, loss_mask, targets, phases, shapes, wav_lens, stems, track_names, starts in dl:\n","        # --- token CE per-item (masked) ---\n","        with torch.no_grad():\n","            logits, _ = model(tokens, loss_mask=None)  # we’ll compute per-item CE manually\n","        # prepare shifted views once\n","        logits_shift = logits[:, :-1, :].contiguous()\n","        targets_shift = tokens[:, 1:].contiguous()\n","        mask_shift = loss_mask[:, 1:].contiguous()\n","\n","        preds = decode_batch(model, tokens, phases, shapes, wav_lens, stems)\n","        for i, pred in enumerate(preds):\n","            tgt = targets[i]  # (T,)\n","            d = float(si_sdr(tgt, pred).item())\n","            total += d; count += 1\n","            per_stem_sum[stems[i]] += d; per_stem_cnt[stems[i]] += 1\n","            # per-item CE: only masked positions\n","            mi = mask_shift[i].view(-1)\n","            nm = int(mi.sum().item())\n","            if nm > 0:\n","                li = logits_shift[i].view(-1, logits.size(-1))[mi]\n","                ti = targets_shift[i].view(-1)[mi]\n","                ce_i = float(F.cross_entropy(li, ti).item())\n","            else:\n","                ce_i = float(\"nan\")\n","            window_rows.append((track_names[i], int(starts[i]), stems[i], d, ce_i, nm))\n","        it += 1\n","        if it >= max_batches:\n","            break\n","    avg = total / max(1, count)\n","    per_stem = {s: (per_stem_sum[s] / max(1, per_stem_cnt[s])) for s in cfg.stems}\n","    print(f\"[SI-SDR] avg over {count} windows: {avg:.3f} dB\")\n","    for s in cfg.stems:\n","        print(f\"  {s:>6}: {per_stem[s]:.3f} dB (n={per_stem_cnt[s]})\")\n","\n","     # ---- CSV export for window-level metrics ----\n","    metrics_root = metrics_dir or \"./metrics\"\n","    os.makedirs(metrics_root, exist_ok=True)\n","    windows_csv = os.path.join(metrics_root, \"sisdr_windows.csv\")\n","    with open(windows_csv, \"w\", newline=\"\") as f:\n","        w = csv.writer(f)\n","        w.writerow([\"track\", \"start_sample\", \"stem\", \"sisdr_db\", \"ce_loss\", \"num_masked_tokens\"])\n","        for (tr, st, stem, sisdr_val, ce_val, nmask) in window_rows:\n","            w.writerow([tr, st, stem,\n","                        f\"{sisdr_val:.3f}\",\n","                        (f\"{ce_val:.5f}\" if ce_val == ce_val else \"\"),  # blank if NaN\n","                        nmask])\n","    print(f\"[csv] wrote {windows_csv}\")\n","    return avg, per_stem\n","\n","def save_checkpoint(model: nn.Module, out_dir: str, tag: str):\n","    os.makedirs(out_dir, exist_ok=True)\n","    path = os.path.join(out_dir, f\"model_{tag}.pt\")\n","    torch.save(model.state_dict(), path)\n","    print(f\"[ckpt] saved: {path}\")\n","    return path\n","\n","def train_model(\n","    dataset_root: str,\n","    out_dir: str = \"./checkpoints\",\n","    epochs: int = 5,\n","    batch_size: int = 4,\n","    lr: float = 3e-4,\n","    overlap: float = 0.5,\n","    num_workers: int = 2,\n","    preview_every: int = 1,\n","    train_tracks: Optional[List[str]] = None,\n","):\n","    \"\"\"\n","    Train the GPT-style separator on a folder-structured dataset.\n","    - Saves checkpoints to out_dir each epoch.\n","    - Prints training loss; optionally writes a preview WAV every few epochs.\n","    \"\"\"\n","    device = cfg.device\n","    print(f\"[train] device={device}\")\n","    model = build_model()\n","    opt = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.1)\n","\n","    if train_tracks is None:\n","        # default: build a split and use the train portion\n","        split = split_dataset(dataset_root, val_fraction=0.1, test_fraction=0.1, seed=42)\n","        train_tracks = split[\"train\"]\n","    ds = StemSeparationDataset(dataset_root, track_dirs=train_tracks, stems=cfg.stems,\n","                               window_seconds=cfg.seconds, overlap=overlap)\n","    if len(ds) == 0:\n","        raise RuntimeError(f\"No training items found under {dataset_root}\")\n","    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n","                                     collate_fn=collate_pack, pin_memory=False)\n","\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        running = 0.0\n","        steps = 0\n","        for tokens, loss_mask, targets, phases, shapes, wav_lens, stems, _, _ in dl:\n","            # Forward\n","            logits, loss = model(tokens, loss_mask=loss_mask)\n","            opt.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            opt.step()\n","            running += float(loss.item())\n","            steps += 1\n","            if steps % 50 == 0:\n","                print(f\"[epoch {epoch}] step {steps} loss {running/steps:.4f}\")\n","\n","        avg_loss = running / max(1, steps)\n","        print(f\"[epoch {epoch}] avg training loss: {avg_loss:.4f}\")\n","        save_checkpoint(model, out_dir, tag=f\"epoch{epoch:03d}\")\n","\n","        # Optional preview decode on the last seen batch\n","        if preview_every and (epoch % preview_every == 0):\n","            model.eval()\n","            with torch.no_grad():\n","                preds = decode_batch(model, tokens, phases, shapes, wav_lens, stems)\n","            # write first item preview\n","            preview_dir = os.path.join(out_dir, \"previews\")\n","            os.makedirs(preview_dir, exist_ok=True)\n","            torchaudio.save(os.path.join(preview_dir, f\"epoch{epoch:03d}_pred.wav\"),\n","                            preds[0].unsqueeze(0).cpu(), cfg.sr)\n","            print(f\"[preview] wrote {os.path.join(preview_dir, f'epoch{epoch:03d}_pred.wav')}\")\n","\n","    print(\"[train] done\")\n","    return model\n","\n","def validate_model(\n","    dataset_root: str,\n","    ckpt: str,\n","    batch_size: int = 4,\n","    overlap: float = 0.5,\n","    num_workers: int = 2,\n","    max_batches: int = 50,\n","    val_tracks: Optional[List[str]] = None,\n","):\n","    \"\"\"\n","    Quick validation: reports average token CE loss on windows from the dataset.\n","    (Faster than full audio SDR; you can add SI-SDR later if desired.)\n","    \"\"\"\n","    model = build_model()\n","    load_checkpoint(model, ckpt)\n","    if val_tracks is None:\n","        split = split_dataset(dataset_root, val_fraction=0.1, test_fraction=0.1, seed=42)\n","        val_tracks = split[\"val\"]\n","    ds = StemSeparationDataset(dataset_root, track_dirs=val_tracks, stems=cfg.stems,\n","                               window_seconds=cfg.seconds, overlap=overlap)\n","    if len(ds) == 0:\n","        raise RuntimeError(f\"No validation items found under {dataset_root}\")\n","    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n","                                     collate_fn=collate_pack, pin_memory=False)\n","    model.eval()\n","    total, count = 0.0, 0\n","    with torch.no_grad():\n","        for i, (tokens, loss_mask, targets, _, _, _, _, _, _) in enumerate(dl):\n","            _, loss = model(tokens, loss_mask=loss_mask)\n","            total += float(loss.item())\n","            count += 1\n","            if i+1 >= max_batches:\n","                break\n","    avg = total / max(1, count)\n","    print(f\"[val] avg CE loss over {count} batches: {avg:.4f}\")\n","    return avg\n","\n","@torch.no_grad()\n","def evaluate_sisdr_tracks(\n","    dataset_root: str,\n","    ckpt: str,\n","    stems: Optional[List[str]] = None,\n","    overlap: float = 0.5,\n","    out_dir: Optional[str] = None,\n","    track_dirs: Optional[List[str]] = None,\n","    metrics_dir: Optional[str] = None,\n","):\n","    \"\"\"\n","    Track-level SI-SDR evaluation.\n","    - Separates each full track's mixture with the trained model.\n","    - Computes SI-SDR for each requested stem against its reference stem file.\n","    - Aggregates per-stem and overall averages; returns (avg_all, per_stem_avg, per_track_dict).\n","\n","    Directory layout per track:\n","        <track_dir>/\n","          mixture.wav  (or mix.wav)\n","          vocals.wav\n","          drums.wav\n","          bass.wav\n","          other.wav\n","\n","    Args:\n","      dataset_root: root folder containing track subfolders.\n","      ckpt: path to model checkpoint (state_dict).\n","      stems: list of stems to evaluate (defaults to cfg.stems).\n","      overlap: window overlap for separation (0..1).\n","      out_dir: if provided, write predicted stems to out_dir/<track_name>/<stem>.wav\n","      track_dirs: explicit list of track directories to evaluate (defaults to test split).\n","      metrics_dir: where to write CSV metrics. If None, uses out_dir if given,\n","                   otherwise writes to a new \"./metrics\" folder.\n","                   Produces: sisdr_per_stem.csv, sisdr_per_track.csv\n","    \"\"\"\n","    stems = stems or cfg.stems\n","    # Build model\n","    model = build_model()\n","    load_checkpoint(model, ckpt)\n","\n","    # Select tracks\n","    if track_dirs is None:\n","        split = split_dataset(dataset_root, val_fraction=0.1, test_fraction=0.1, seed=42)\n","        track_dirs = split[\"test\"]\n","        if len(track_dirs) == 0:\n","            # Fall back to 'val' if test empty\n","            track_dirs = split[\"val\"]\n","    if len(track_dirs) == 0:\n","        raise RuntimeError(\"No tracks available for evaluation.\")\n","\n","    # Accumulators\n","    per_stem_sum: Dict[str, float] = {s: 0.0 for s in stems}\n","    per_stem_cnt: Dict[str, int] = {s: 0     for s in stems}\n","    per_track_scores: Dict[str, Dict[str, float]] = {}\n","    name_map = _default_stem_files()  # stem -> subdir\n","\n","    if out_dir is not None:\n","        os.makedirs(out_dir, exist_ok=True)\n","\n","    # Iterate tracks\n","    for track_dir in track_dirs:\n","        track_name = os.path.basename(track_dir.rstrip(\"/\\\\\"))\n","       # Build full-file mixture by summing stems (no mixture.wav in this layout)\n","        # Determine max length across available stems first\n","        max_len = 0\n","        stem_paths = {}\n","        for s in stems:\n","            subdir = name_map.get(s, None)\n","            sp = _find_stem_wav_from_subdir(track_dir, subdir)\n","            stem_paths[s] = sp\n","            if sp is None:\n","                continue\n","            info = torchaudio.info(sp)\n","            frames_at_target = int(round(info.num_frames * (cfg.sr / info.sample_rate)))\n","            max_len = max(max_len, frames_at_target)\n","        if max_len == 0:\n","            print(f\"[warn] {track_name}: no usable stems, skipping.\")\n","            continue\n","        mix = torch.zeros(max_len, dtype=torch.float32, device=cfg.device)\n","        for s in stems:\n","            sp = stem_paths.get(s, None)\n","            if sp is None:\n","                continue\n","            wav = load_wav(sp, cfg.sr).to(cfg.device)\n","            wav = mono(wav)\n","            wav = pad_or_trim(wav, max_len)\n","            mix += wav\n","        # Predict stems (full-file)\n","        preds = separate_stems_from_mix(model, mix, stems=stems, overlap=overlap)\n","\n","        # Optionally save predictions\n","        if out_dir is not None:\n","            tdir = os.path.join(out_dir, track_name)\n","            os.makedirs(tdir, exist_ok=True)\n","            for s in stems:\n","                torchaudio.save(os.path.join(tdir, f\"{s.lower()}.wav\"),\n","                                preds[s].unsqueeze(0).cpu(), cfg.sr)\n","\n","        # Score each requested stem\n","        track_scores: Dict[str, float] = {}\n","        for s in stems:\n","            ref_dir = name_map.get(s, None)\n","            if ref_dir is None:\n","                print(f\"[warn] {track_name}: no filename mapping for stem '{s}', skipping.\")\n","                continue\n","            ref_path = _find_stem_wav_from_subdir(track_dir, ref_dir)\n","            if ref_path is None:\n","                print(f\"[warn] {track_name}: missing reference subdir '{ref_dir}' for stem '{s}', skipping.\")\n","                continue\n","            ref = load_wav(ref_path, cfg.sr).to(cfg.device)\n","            ref = mono(ref)\n","            # Align lengths to mixture length\n","            T = mix.numel()\n","            ref = pad_or_trim(ref, T)\n","            pred = preds[s][:T]\n","            d = float(si_sdr(ref, pred).item())\n","\n","            track_scores[s] = d\n","            per_stem_sum[s] += d\n","            per_stem_cnt[s] += 1\n","\n","        per_track_scores[track_name] = track_scores\n","        # Print per-track summary\n","        if len(track_scores) > 0:\n","            stem_list = \", \".join([f\"{k}:{v:.2f}dB\" for k, v in track_scores.items()])\n","            print(f\"[track] {track_name}: {stem_list}\")\n","        else:\n","            print(f\"[track] {track_name}: no scored stems.\")\n","\n","    # Reduce\n","    total_sum = sum(per_stem_sum.values())\n","    total_cnt = sum(per_stem_cnt.values())\n","    avg_all = total_sum / max(1, total_cnt)\n","    per_stem_avg = {s: (per_stem_sum[s] / max(1, per_stem_cnt[s])) for s in stems}\n","\n","    print(f\"[SI-SDR/tracks] overall avg (all stems, all tracks): {avg_all:.3f} dB\")\n","    for s in stems:\n","        print(f\"  {s:>6}: {per_stem_avg[s]:.3f} dB (n={per_stem_cnt[s]})\")\n","     # -------- CSV export --------\n","    # Decide metrics directory\n","    metrics_root = metrics_dir or out_dir or \"./metrics\"\n","    os.makedirs(metrics_root, exist_ok=True)\n","\n","    # 1) Per-stem CSV\n","    per_stem_csv = os.path.join(metrics_root, \"sisdr_per_stem.csv\")\n","    with open(per_stem_csv, \"w\", newline=\"\") as f:\n","        w = csv.writer(f)\n","        w.writerow([\"stem\", \"avg_sisdr_db\", \"n_scored\"])\n","        for s in stems:\n","            w.writerow([s, f\"{per_stem_avg[s]:.3f}\", per_stem_cnt[s]])\n","    print(f\"[csv] wrote {per_stem_csv}\")\n","\n","    # 2) Per-track CSV (one row per track, columns for each stem)\n","    per_track_csv = os.path.join(metrics_root, \"sisdr_per_track.csv\")\n","    header = [\"track\"] + stems + [\"num_scored_stems\", \"avg_sisdr_db\"]\n","    with open(per_track_csv, \"w\", newline=\"\") as f:\n","        w = csv.writer(f)\n","        w.writerow(header)\n","        for track_name, scores in per_track_scores.items():\n","            vals = []\n","            present = []\n","            for s in stems:\n","                if s in scores:\n","                    vals.append(f\"{scores[s]:.3f}\")\n","                    present.append(scores[s])\n","                else:\n","                    vals.append(\"\")  # empty if missing\n","            n_present = len(present)\n","            row_avg = sum(present) / n_present if n_present > 0 else float(\"nan\")\n","            w.writerow([track_name] + vals + [n_present, (f\"{row_avg:.3f}\" if n_present > 0 else \"\")])\n","    print(f\"[csv] wrote {per_track_csv}\")\n","\n","    return avg_all, per_stem_avg, per_track_scores\n","\n","# =========================\n","# Notebook-friendly entry points (no argparse)\n","# =========================\n","def init_model(ckpt: str | None = None):\n","    \"\"\"Build the model and optionally load a checkpoint. Returns the model.\"\"\"\n","    torch.manual_seed(0)\n","    device = cfg.device\n","    print(\"Device:\", device)\n","    model = build_model()\n","    load_checkpoint(model, ckpt)\n","    return model\n","\n","def run_separation(mix_path: str, out_dir: str = \".\", stems: list[str] | None = None,\n","                   overlap: float = 0.5, ckpt: str | None = None):\n","    \"\"\"\n","    Separate a mixed WAV file into stems and write WAVs to out_dir.\n","    Jupyter usage:\n","        model = init_model(ckpt=\"/path/to/model.pt\")\n","        run_separation(\"/path/to/mix.wav\", out_dir=\"./out\", stems=[\"VOCALS\",\"DRUMS\"], overlap=0.5, ckpt=None)\n","    If ckpt is provided here, it will be loaded inside (you can also pass a pre-loaded model).\n","    \"\"\"\n","    model = init_model(ckpt) if ckpt is not None else build_model()\n","    if ckpt is None:\n","        print(\"[warn] No checkpoint provided. Outputs will likely sound noisy (untrained weights).\")\n","    stems_arg = stems or cfg.stems\n","    separate_file(model, mix_path, out_dir, stems=stems_arg, overlap=overlap)\n","\n","def run_demo(output_dir: str = \"/content/drive/MyDrive/MSS_Audio/\"):\n","    \"\"\"\n","    Run the synthetic demo (440 Hz sine + noise) and write debugging WAVs.\n","    Jupyter usage:\n","        run_demo(output_dir=\"./demo_out\")\n","    \"\"\"\n","    torch.manual_seed(0)\n","    device = cfg.device\n","    print(\"Device:\", device)\n","\n","    # Build a fresh (random) model; demo_step runs one toy optimization step\n","    model = build_model()\n","\n","    # Backwards-compatible toy demo (synthetic 440 Hz sine + noise)\n","    T = int(cfg.seconds * cfg.sr)\n","    t = torch.linspace(0, cfg.seconds, T, dtype=torch.float32)\n","    sine = 0.5 * torch.sin(2 * math.pi * 440 * t)\n","    noise = 0.1 * torch.randn_like(sine)\n","    vocals = sine\n","    other = noise\n","    mix = (vocals + other).to(device)\n","\n","    # Pad/trim exactly cfg.seconds\n","    mix = pad_or_trim(mix, T)\n","    vocals = pad_or_trim(vocals.to(device), T)\n","\n","    opt = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n","    loss, pred = demo_step(model, opt, mix, vocals, stem_name=\"VOCALS\")\n","    print(\"Loss:\", loss)\n","\n","    # Save predicted wav for inspection\n","    path = \"/content/drive/MyDrive/MSS_Audio/\"\n","    os.makedirs(path, exist_ok=True)\n","    torchaudio.save(path + \"pred_vocals_demo.wav\", pred.unsqueeze(0).cpu(), cfg.sr)\n","    print(\"Wrote pred_vocals_demo.wav\")\n","\n","    # === Ground-truth recon sanity checks ===\n","    # 1) Save the clean target (should be a pure 440 Hz tone)\n","    torchaudio.save(path + \"target_vocals_ground_truth.wav\", vocals.unsqueeze(0).cpu(), cfg.sr)\n","    print(\"Wrote target_vocals_ground_truth.wav\")\n","\n","    # 2) Quantize→dequantize the stem and reconstruct with STEM PHASE (cleanest)\n","    with torch.no_grad():\n","        stem_mag_lin_dbg, stem_phase_dbg = stft_mag_phase(vocals)\n","        if cfg.use_mel:\n","            stem_feat_dbg = linear_to_mel(stem_mag_lin_dbg)\n","        else:\n","            stem_feat_dbg = stem_mag_lin_dbg\n","        stem_log_dbg = to_logmag(stem_feat_dbg)\n","        stem_tok_dbg = quantize_logmag(stem_log_dbg)\n","        deq_feat_dbg = dequantize_to_mag(stem_tok_dbg, stem_feat_dbg.shape).to(stem_phase_dbg.device)\n","        if cfg.use_mel:\n","            deq_mag_lin_dbg = mel_to_linear(deq_feat_dbg)\n","        else:\n","            deq_mag_lin_dbg = deq_feat_dbg\n","        recon_gt_stemphase = istft_from_mag_phase(deq_mag_lin_dbg, stem_phase_dbg, T).clamp(-1, 1)\n","    torchaudio.save(path + \"recon_from_gt_tokens.wav\", recon_gt_stemphase.unsqueeze(0).cpu(), cfg.sr)\n","    print(\"Wrote recon_from_gt_tokens.wav\")\n","\n","    # 3) Same dequantized magnitude but use MIXTURE PHASE (slightly dirtier)\n","    with torch.no_grad():\n","        mix_mag_lin_dbg, mix_phase_dbg = stft_mag_phase(mix)\n","        recon_gt_mixphase = istft_from_mag_phase(deq_mag_lin_dbg, mix_phase_dbg, T).clamp(-1, 1)\n","    torchaudio.save(path + \"recon_from_gt_tokens_mixphase.wav\", recon_gt_mixphase.unsqueeze(0).cpu(), cfg.sr)\n","    print(\"Wrote recon_from_gt_tokens_mixphase.wav\")"]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/MSS_Audio/moisesdb\"\n","# 0) (Optional) make and reuse a split\n","split = split_dataset(path + \"/dataset_root\", val_fraction=0.1, test_fraction=0.1, seed=42)\n","\n","# 1) Train on the train split\n","train_model(\n","    dataset_root=path + \"/dataset_root\",\n","    out_dir=path + \"/checkpoints\",\n","    epochs=5,\n","    batch_size=4,\n","    overlap=0.5,\n","    train_tracks=split[\"train\"],   # or omit to auto-split\n",")\n","\n","# 2) Quick validation (token CE)\n","validate_model(\n","    dataset_root=path + \"/dataset_root\",\n","    ckpt=path + \"/checkpoints/model_epoch005.pt\",\n","    batch_size=4,\n","    val_tracks=split[\"val\"],       # or omit to auto-split\n",")\n","\n","# 3) SI-SDR evaluation with decoding (heavier but more meaningful)\n","evaluate_sisdr(\n","    dataset_root=path + \"/dataset_root\",\n","    ckpt=path + \"/checkpoints/model_epoch005.pt\",\n","    batch_size=2,\n","    max_batches=100,\n","    track_dirs=split_dataset(path + \"/dataset_root\")[\"val\"],\n","    metrics_dir=\"./metrics_val\"\n",")\n","\n","# 4) Inference on a full mix file\n","model = init_model(ckpt=path + \"/checkpoints/model_epoch005.pt\")\n","separate_file(model, mix_path=\"./some_mix.wav\", out_dir=\"./stems_out\",\n","                  stems=[\"VOCALS\",\"DRUMS\",\"BASS\",\"OTHER\"], overlap=0.5)\n","\n","# Evaluate on your test split, save predicted stems and CSVs\n","avg_all, per_stem, per_track = evaluate_sisdr_tracks(\n","    dataset_root=path + \"/dataset_root\",\n","    ckpt=path + \"/checkpoints/model_epoch005.pt\",\n","    stems=[\"VOCALS\",\"DRUMS\",\"BASS\",\"GUITAR\"],\n","    overlap=0.5,\n","    out_dir=\"./preds_test\",        # optional: writes audio here\n","    metrics_dir=\"./metrics_test\",  # CSVs will be written here\n","    track_dirs=split_dataset(path + \"/dataset_root\")[\"test\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bL3y0yPlUsd_","outputId":"8ac4d290-36e5-415b-e44b-62e4eebb567b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[split] total=10 → train=8, val=1, test=1\n","[train] device=cuda\n"]}]}]}