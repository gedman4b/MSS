{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHCnfJvPyOkg",
        "outputId": "94bbd30e-35d7-4a04-8f83-648bff286768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "[debug] loss_mask target tokens this step: 0\n",
            "Loss: 0.0\n",
            "Wrote mixture_test.wav\n",
            "Wrote pred_vocals_demo.wav\n",
            "[debug] loss_mask target tokens this step: 0\n",
            "Square Loss: 0.0\n",
            "Wrote pred_square_demo.wav\n",
            "Wrote target_vocals_ground_truth.wav\n",
            "Wrote recon_from_gt_tokens.wav\n",
            "Wrote recon_from_gt_tokens_mixphase.wav\n",
            "Wrote target_square_ground_truth.wav\n",
            "Wrote square_recon_from_gt_tokens.wav\n",
            "Wrote square_recon_from_gt_tokens_mixphase.wav\n"
          ]
        }
      ],
      "source": [
        "# sep_gpt_v0.py\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "# =========================\n",
        "# Config (minimal recipe)\n",
        "# =========================\n",
        "class Cfg:\n",
        "    sr = 44100\n",
        "    n_fft = 1024\n",
        "    hop = 256             # 75% overlap\n",
        "    win = 1024\n",
        "    pad = 0\n",
        "    center = True\n",
        "    power = 1.0           # magnitude (not power) for STFT\n",
        "    # Tokenization range on log10 magnitude (clipped)\n",
        "    logmag_min_db = -12.0  # ~ -12 dB relative floor after per-window normalize\n",
        "    logmag_max_db = 2.0\n",
        "    n_bits = 8            # 8-bit uniform quant\n",
        "    # Model\n",
        "    d_model = 512\n",
        "    n_heads = 8\n",
        "    n_layers = 8\n",
        "    ff_mult = 4\n",
        "    dropout = 0.1\n",
        "    # Windows\n",
        "    seconds = 0.1  # Reduced seconds for smaller sequence\n",
        "    max_seq_len = 4096 # Further reduced max sequence length config\n",
        "    # Stems\n",
        "    stems = [\"VOCALS\", \"SQUARE\", \"DRUMS\", \"BASS\", \"OTHER\"]  # added 'SQUARE' stem\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = Cfg()\n",
        "\n",
        "# =========================\n",
        "# Helper: STFT / iSTFT\n",
        "# =========================\n",
        "def stft_mag_phase(wav: torch.Tensor):\n",
        "    \"\"\"\n",
        "    wav: (T,) mono tensor, float32 [-1,1]\n",
        "    returns: mag (frames, freq), phase (frames, freq)\n",
        "    \"\"\"\n",
        "    window = torch.hann_window(cfg.win, device=wav.device)\n",
        "    stft = torch.stft(\n",
        "        wav,\n",
        "        n_fft=cfg.n_fft,\n",
        "        hop_length=cfg.hop,\n",
        "        win_length=cfg.win,\n",
        "        window=window,\n",
        "        center=cfg.center,\n",
        "        return_complex=True,\n",
        "        pad_mode=\"reflect\",\n",
        "    )  # (freq, frames)\n",
        "    spec = stft.transpose(0,1)          # (frames, freq)\n",
        "    mag = spec.abs()\n",
        "    phase = torch.angle(spec)\n",
        "    return mag, phase\n",
        "\n",
        "def istft_from_mag_phase(mag: torch.Tensor, phase: torch.Tensor, length: int):\n",
        "    \"\"\"\n",
        "    mag/phase: (frames, freq)\n",
        "    length: samples to trim/pad to\n",
        "    \"\"\"\n",
        "    spec = (mag * torch.exp(1j*phase)).transpose(0,1)  # (freq, frames)\n",
        "    window = torch.hann_window(cfg.win, device=mag.device)\n",
        "    wav = torch.istft(\n",
        "        spec,\n",
        "        n_fft=cfg.n_fft,\n",
        "        hop_length=cfg.hop,\n",
        "        win_length=cfg.win,\n",
        "        window=window,\n",
        "        center=cfg.center,\n",
        "        length=length,\n",
        "    )\n",
        "    return wav\n",
        "\n",
        "# =========================\n",
        "# Quantization (log-mag)\n",
        "# =========================\n",
        "def to_logmag(mag: torch.Tensor, eps=1e-8):\n",
        "    # Per-window normalize: scale by max to stabilize, then log10\n",
        "    m = torch.clamp(mag / (mag.max() + eps), min=eps)\n",
        "    logm = torch.log10(m)\n",
        "    # map to \"dB-ish\" range by multiplying 20 (optional), but here we treat log10 directly.\n",
        "    # We'll just clamp in fixed range (already tuned for normalized mags).\n",
        "    logm = torch.clamp(logm, math.log10(10**(cfg.logmag_min_db/20)),\n",
        "                             math.log10(10**(cfg.logmag_max_db/20)))\n",
        "    return logm\n",
        "\n",
        "def quantize_logmag(logm: torch.Tensor):\n",
        "    \"\"\"\n",
        "    logm: (frames, freq)\n",
        "    returns: tokens (frames*freq,)\n",
        "    \"\"\"\n",
        "    lo = math.log10(10**(cfg.logmag_min_db/20))\n",
        "    hi = math.log10(10**(cfg.logmag_max_db/20))\n",
        "    qlevels = (1 << cfg.n_bits)\n",
        "    x = (logm - lo) / (hi - lo)  # [0,1]\n",
        "    x = torch.clamp(x, 0, 1)\n",
        "    q = torch.round(x * (qlevels - 1)).to(torch.long)  # [0..255]\n",
        "    return q.view(-1)\n",
        "\n",
        "def dequantize_to_mag(tokens: torch.Tensor, shape):\n",
        "    \"\"\"\n",
        "    tokens: (frames*freq,)\n",
        "    shape: (frames, freq)\n",
        "    returns: mag (frames, freq)\n",
        "    \"\"\"\n",
        "    lo = math.log10(10**(cfg.logmag_min_db/20))\n",
        "    hi = math.log10(10**(cfg.logmag_max_db/20))\n",
        "    qlevels = (1 << cfg.n_bits)\n",
        "    x = tokens.float() / (qlevels - 1)\n",
        "    logm = x * (hi - lo) + lo\n",
        "    m = 10 ** logm  # invert log10 amplitude\n",
        "    return m.view(*shape)\n",
        "\n",
        "# =========================\n",
        "# Token vocab & packing\n",
        "# =========================\n",
        "class Vocab:\n",
        "    # quantized bins 0..255\n",
        "    # special/control tokens appended after\n",
        "    PAD = 256\n",
        "    BOS = 257\n",
        "    EOS = 258\n",
        "    SEP = 259\n",
        "    MIX = 260\n",
        "    STEM_BASE = 300  # STEM tokens at STEM_BASE + idx\n",
        "\n",
        "    def __init__(self):\n",
        "        self.num_bins = 1 << cfg.n_bits\n",
        "        self.stem_to_id = {name: self.STEM_BASE + i for i, name in enumerate(cfg.stems)}\n",
        "        self.id_to_stem = {v:k for k,v in self.stem_to_id.items()}\n",
        "        self.vocab_size = self.STEM_BASE + len(cfg.stems)\n",
        "\n",
        "vocab = Vocab()\n",
        "\n",
        "def pack_sequence(mix_tokens: torch.Tensor, stem_name: str, target_tokens: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Create LM sequence:\n",
        "    [BOS, MIX, mix..., SEP, STEM(token), target..., EOS]\n",
        "    Returns tokens (L,), loss_mask (L,) where mask=1 for target positions only.\n",
        "    \"\"\"\n",
        "    stem_tok = torch.tensor([vocab.stem_to_id[stem_name]], device=mix_tokens.device, dtype=torch.long)\n",
        "    seq = torch.cat([\n",
        "        torch.tensor([vocab.BOS, vocab.MIX], device=mix_tokens.device),\n",
        "        mix_tokens,\n",
        "        torch.tensor([vocab.SEP], device=mix_tokens.device),\n",
        "        stem_tok,\n",
        "        target_tokens,\n",
        "        torch.tensor([vocab.EOS], device=mix_tokens.device)\n",
        "    ])\n",
        "    # Loss mask: predict from position after STEM token inclusive\n",
        "    loss_mask = torch.zeros_like(seq, dtype=torch.bool)\n",
        "    # positions where target tokens + EOS reside\n",
        "    start = (2 + mix_tokens.numel() + 1 + 1)  # after BOS,MIX + mix + SEP + STEM\n",
        "    loss_mask[start:] = True\n",
        "    return seq, loss_mask\n",
        "\n",
        "def split_mix_target_from_seq(seq: torch.Tensor):\n",
        "    # helper for debugging; not needed in training loop\n",
        "    pass\n",
        "\n",
        "# =========================\n",
        "# GPT-style decoder-only model\n",
        "# =========================\n",
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_mult, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_mult*d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_mult*d_model, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x, attn_mask):\n",
        "        h = self.ln1(x)\n",
        "        # attn_mask should have shape (B, L, L) or (L, L) when batch_first=True\n",
        "        # We pass (B, L, L) from GPT forward, potentially (B * n_heads, L, L)\n",
        "        y, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
        "        x = x + y\n",
        "        h = self.ln2(x)\n",
        "        x = x + self.ff(h)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, ff_mult, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        # Use cfg.max_seq_len for positional embedding\n",
        "        self.pos = nn.Parameter(torch.zeros(1, cfg.max_seq_len, d_model))\n",
        "        self.blocks = nn.ModuleList([\n",
        "            GPTBlock(d_model, n_heads, ff_mult, dropout) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "        self.n_heads = n_heads # Store n_heads for mask creation\n",
        "\n",
        "    def forward(self, tokens, loss_mask=None):\n",
        "        \"\"\"\n",
        "        tokens: (B, L) long\n",
        "        loss_mask: (B, L) bool (optional)\n",
        "        returns: logits (B, L, V), loss (scalar) if mask provided\n",
        "        \"\"\"\n",
        "        B, L = tokens.shape\n",
        "        max_pos = self.pos.size(1)\n",
        "        if L > max_pos:\n",
        "            tokens = tokens[:, :max_pos]\n",
        "            if loss_mask is not None:\n",
        "                loss_mask = loss_mask[:, :max_pos]\n",
        "            L = max_pos\n",
        "        x = self.embed(tokens) + self.pos[:, :L, :]\n",
        "        # causal mask: no lookahead\n",
        "        causal = torch.triu(torch.ones(L, L, device=tokens.device, dtype=torch.bool), diagonal=1)\n",
        "        # Repeat causal mask for each head\n",
        "        causal = causal.unsqueeze(0).repeat(B * self.n_heads, 1, 1)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, causal)\n",
        "        x = self.ln(x)\n",
        "        logits = self.out(x)\n",
        "        loss = None\n",
        "        if loss_mask is not None:\n",
        "            # shift targets by one (next token prediction)\n",
        "            logits_shift = logits[:, :-1, :].contiguous()\n",
        "            targets = tokens[:, 1:].contiguous()\n",
        "            mask = loss_mask[:, 1:].contiguous()\n",
        "            # Only compute loss if mask selects at least one position\n",
        "            if mask.view(-1).sum() > 0:\n",
        "                loss = F.cross_entropy(\n",
        "                    logits_shift.view(-1, logits.size(-1))[mask.view(-1)],\n",
        "                    targets.view(-1)[mask.view(-1)]\n",
        "                )\n",
        "            else:\n",
        "                loss = torch.tensor(0.0, device=logits.device)\n",
        "        return logits, loss\n",
        "\n",
        "# =========================\n",
        "# Data demo utilities\n",
        "# =========================\n",
        "def pad_or_trim(wav, length):\n",
        "    T = wav.numel()\n",
        "    if T >= length:\n",
        "        return wav[:length]\n",
        "    out = torch.zeros(length, device=wav.device)\n",
        "    out[:T] = wav\n",
        "    return out\n",
        "\n",
        "def mono(wav):\n",
        "    if wav.dim() == 2:\n",
        "        return wav.mean(0)\n",
        "    return wav\n",
        "\n",
        "def load_wav(path, target_sr):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    wav = mono(wav)\n",
        "    if sr != target_sr:\n",
        "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
        "    wav = wav.clamp(-1, 1)\n",
        "    return wav\n",
        "\n",
        "# =========================\n",
        "# End-to-end: one batch example\n",
        "# =========================\n",
        "def make_batch(mix_wav, stem_wav, stem_name: str):\n",
        "    \"\"\"\n",
        "    Build a (1, L) token batch and loss mask for one 2s window.\n",
        "    \"\"\"\n",
        "    device = cfg.device\n",
        "    # STFTs\n",
        "    mix_mag, mix_phase = stft_mag_phase(mix_wav)\n",
        "    stem_mag, _ = stft_mag_phase(stem_wav)\n",
        "\n",
        "    # Tokenize\n",
        "    mix_log = to_logmag(mix_mag)\n",
        "    stem_log = to_logmag(stem_mag)\n",
        "    mix_tok = quantize_logmag(mix_log)\n",
        "    stem_tok = quantize_logmag(stem_log)\n",
        "\n",
        "    # Pack\n",
        "    seq, mask = pack_sequence(mix_tok, stem_name, stem_tok)\n",
        "\n",
        "    # Trim to max_seq_len\n",
        "    if seq.numel() > cfg.max_seq_len:\n",
        "        seq = seq[:cfg.max_seq_len]\n",
        "        mask = mask[:cfg.max_seq_len]\n",
        "\n",
        "    return seq.unsqueeze(0).to(device), mask.unsqueeze(0).to(device), mix_phase, mix_mag.shape\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(model, mix_tokens, stem_name: str, max_new=None):\n",
        "    \"\"\"\n",
        "    mix_tokens: (Lmix,) long\n",
        "    Returns full sequence including prefix+generated tokens.\n",
        "    \"\"\"\n",
        "    device = cfg.device\n",
        "    stem_tok = torch.tensor([vocab.stem_to_id[stem_name]], device=device, dtype=torch.long)\n",
        "    seq = torch.cat([\n",
        "        torch.tensor([vocab.BOS, vocab.MIX], device=device),\n",
        "        mix_tokens.to(device),\n",
        "        torch.tensor([vocab.SEP], device=device),\n",
        "        stem_tok\n",
        "    ])\n",
        "    seq = seq.unsqueeze(0)  # (1, L)\n",
        "    # generate until EOS or max_new\n",
        "    max_new = max_new or (mix_tokens.numel() + 1024)  # cap\n",
        "    # Cap generation length at max_seq_len\n",
        "    max_new = min(max_new, cfg.max_seq_len - seq.size(1))\n",
        "    for _ in range(max_new):\n",
        "        logits, _ = model(seq)\n",
        "        next_logits = logits[:, -1, :]\n",
        "        next_tok = torch.argmax(next_logits, dim=-1)  # shape: (1,)\n",
        "        seq = torch.cat([seq, next_tok.unsqueeze(1)], dim=1)\n",
        "        if int(next_tok.item()) == vocab.EOS:\n",
        "            break\n",
        "    return seq.squeeze(0)\n",
        "\n",
        "\n",
        "def reconstruct_from_sequence(seq, mix_phase, spec_shape, target_len):\n",
        "    \"\"\"\n",
        "    Extract predicted target tokens from seq and reconstruct WAV via mixture phase.\n",
        "    \"\"\"\n",
        "    # Locate SEP index\n",
        "    sep_pos = (seq == vocab.SEP).nonzero(as_tuple=True)[0]\n",
        "    if sep_pos.numel() == 0:\n",
        "        print(\"Warning: SEP token not found in sequence. Reconstruction may be incomplete.\")\n",
        "        # If SEP is not found, assume the entire sequence after the prefix is target tokens\n",
        "        # (2 for BOS, MIX + mix_tokens length)\n",
        "        prefix_len = 2 + (mix_phase.shape[0] * mix_phase.shape[1]) # Approximate mix token length from spec shape\n",
        "        start = prefix_len + 1 + 1 # after BOS, MIX, mix_tokens, SEP, STEM\n",
        "        start = min(start, seq.numel()) # Ensure start is within bounds\n",
        "        predicted_tokens_segment = seq[start:]\n",
        "    else:\n",
        "        sep_idx = sep_pos[-1].item()\n",
        "        start = sep_idx + 2  # skip SEP and STEM token\n",
        "\n",
        "        # Collect predicted tokens until EOS or end of sequence\n",
        "        eos_pos = (seq[start:] == vocab.EOS).nonzero(as_tuple=True)[0]\n",
        "        if eos_pos.numel() > 0:\n",
        "            end = start + int(eos_pos[0].item())\n",
        "        else:\n",
        "            end = len(seq)\n",
        "        predicted_tokens_segment = seq[start:end]\n",
        "\n",
        "    # Determine the expected length of the target token sequence\n",
        "    expected_len = spec_shape[0] * spec_shape[1]\n",
        "\n",
        "    # Explicitly pad or trim the predicted tokens to the expected length\n",
        "    if predicted_tokens_segment.numel() < expected_len:\n",
        "        pad_len = expected_len - predicted_tokens_segment.numel()\n",
        "        # Pad with a valid quantized bin (e.g., 0 for lowest magnitude)\n",
        "        tgt_tokens_padded = torch.cat([predicted_tokens_segment, torch.full((pad_len,), 0, device=predicted_tokens_segment.device, dtype=torch.long)])\n",
        "    else:\n",
        "        tgt_tokens_padded = predicted_tokens_segment[:expected_len] # Trim if longer\n",
        "\n",
        "    # Ensure the tensor is detached before dequantization if it's not already\n",
        "    tgt_tokens_padded = tgt_tokens_padded.detach()\n",
        "\n",
        "    # Dequantize to magnitude\n",
        "    pred_mag = dequantize_to_mag(tgt_tokens_padded, spec_shape).to(mix_phase.device)\n",
        "\n",
        "    # Use mixture phase for reconstruction\n",
        "    wav = istft_from_mag_phase(pred_mag, mix_phase, target_len)\n",
        "    return wav.clamp(-1, 1)\n",
        "\n",
        "\n",
        "def demo_step(model, optimizer, mix_wav, stem_wav, stem_name=\"VOCALS\"):\n",
        "    \"\"\"\n",
        "    Runs one supervised step and a greedy decode.\n",
        "    \"\"\"\n",
        "    # Build batch\n",
        "    seq, mask, mix_phase, spec_shape = make_batch(mix_wav, stem_wav, stem_name)\n",
        "    # Visibility into how many target tokens are being trained this step\n",
        "    mask_tokens = int(mask.sum().item())\n",
        "    print(f\"[debug] loss_mask target tokens this step: {mask_tokens}\")\n",
        "    # Ensure we actually have a training signal (with our shorter STFT, we should)\n",
        "    # assert mask_tokens > 0, \"Loss mask is empty—target tokens were entirely truncated. Increase max_seq_len or cut STFT/tokenization further.\"\n",
        "\n",
        "    model.train()\n",
        "    logits, loss = model(seq, loss_mask=mask)\n",
        "    optimizer.zero_grad()\n",
        "    # Only call backward if loss requires grad\n",
        "    if loss is not None and loss.requires_grad:\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Greedy decode (eval)\n",
        "    model.eval()\n",
        "    # Slice of mixture tokens must also be capped at max_seq_len\n",
        "    # The mix_tokens slice should match the part of the sequence that represents the mixture tokens\n",
        "    # This is from after BOS and MIX tokens up to the SEP token\n",
        "    sep_pos = (seq[0] == vocab.SEP).nonzero(as_tuple=True)[0]\n",
        "    if sep_pos.numel() > 0:\n",
        "        mix_tok_end = sep_pos[0].item()\n",
        "    else:\n",
        "        # If SEP not found (e.g., trimmed), approximate based on expected mix token length\n",
        "        mix_tok_end = 2 + (spec_shape[0] * spec_shape[1]) # BOS, MIX + mix_tokens\n",
        "        mix_tok_end = min(mix_tok_end, seq.size(1)) # Cap at actual seq length\n",
        "\n",
        "    mix_tok = seq[0, 2:mix_tok_end] # slice after BOS and MIX tokens\n",
        "\n",
        "    with torch.no_grad():\n",
        "        full = greedy_decode(model, mix_tok, stem_name)\n",
        "    # Reconstruct predicted stem\n",
        "    target_len = mix_wav.numel()\n",
        "    pred_wav = reconstruct_from_sequence(full, mix_phase, spec_shape, target_len)\n",
        "    return float(loss.item()) if loss is not None else float('nan'), pred_wav\n",
        "\n",
        "# =========================\n",
        "# Main (toy run)\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    device = cfg.device\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # For a smoke test without a dataset, synthesize a trivial mixture:\n",
        "    # mixture = sine(440Hz) + square(523Hz) + noise\n",
        "    # \"VOCALS\" = sine(440Hz); \"SQUARE\" = square(523Hz)\n",
        "    T = int(cfg.seconds * cfg.sr)\n",
        "    t = torch.linspace(0, cfg.seconds, T, dtype=torch.float32)\n",
        "    sine = 0.5*torch.sin(2*math.pi*440*t)\n",
        "    square = 0.3*torch.sign(torch.sin(2*math.pi*523*t))\n",
        "    noise = 0.1*torch.randn_like(sine)\n",
        "    vocals = sine\n",
        "    # Keep 'other' as just noise; square is now its own stem\n",
        "    other = noise\n",
        "    mix = (vocals + square + other).to(device)\n",
        "\n",
        "    # Pad/trim exactly cfg.seconds\n",
        "    mix = pad_or_trim(mix, T)\n",
        "    vocals = pad_or_trim(vocals.to(device), T)\n",
        "\n",
        "    # Model\n",
        "    model = GPT(\n",
        "        vocab_size=vocab.vocab_size,\n",
        "        d_model=cfg.d_model,\n",
        "        n_layers=cfg.n_layers,\n",
        "        n_heads=cfg.n_heads,\n",
        "        ff_mult=cfg.ff_mult,\n",
        "        dropout=cfg.dropout\n",
        "    ).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "\n",
        "    # One training step + decode\n",
        "    loss, pred = demo_step(model, opt, mix, vocals, stem_name=\"VOCALS\")\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "    path = \"/content/drive/MyDrive/MSS_Audio/\"\n",
        "\n",
        "    torchaudio.save(path + \"mixture_test.wav\", mix.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote mixture_test.wav\")\n",
        "\n",
        "    # Save predicted wav for inspection\n",
        "    torchaudio.save(path + \"pred_vocals_demo.wav\", pred.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote pred_vocals_demo.wav\")\n",
        "\n",
        "    # === Square stem: train+decode and save ===\n",
        "    loss_sq, pred_sq = demo_step(model, opt, mix, square.to(device), stem_name=\"SQUARE\")\n",
        "    print(\"Square Loss:\", loss_sq)\n",
        "    torchaudio.save(path + \"pred_square_demo.wav\", pred_sq.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote pred_square_demo.wav\")\n",
        "\n",
        "    # === Ground-truth recon sanity checks ===\n",
        "    # 1) Save the clean target (should be a pure 440 Hz tone)\n",
        "    torchaudio.save(path + \"target_vocals_ground_truth.wav\", vocals.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote target_vocals_ground_truth.wav\")\n",
        "\n",
        "    # 2) Quantize→dequantize the stem and reconstruct with STEM PHASE (cleanest)\n",
        "    with torch.no_grad():\n",
        "        stem_mag_dbg, stem_phase_dbg = stft_mag_phase(vocals)\n",
        "        stem_log_dbg = to_logmag(stem_mag_dbg)\n",
        "        stem_tok_dbg = quantize_logmag(stem_log_dbg)\n",
        "        deq_mag_dbg = dequantize_to_mag(stem_tok_dbg, stem_mag_dbg.shape).to(stem_phase_dbg.device)\n",
        "        recon_gt_stemphase = istft_from_mag_phase(deq_mag_dbg, stem_phase_dbg, T).clamp(-1, 1)\n",
        "    torchaudio.save(path + \"recon_from_gt_tokens.wav\", recon_gt_stemphase.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote recon_from_gt_tokens.wav\")\n",
        "\n",
        "    # 3) Same dequantized magnitude but use MIXTURE PHASE (slightly dirtier)\n",
        "    with torch.no_grad():\n",
        "        mix_mag_dbg, mix_phase_dbg = stft_mag_phase(mix)\n",
        "        recon_gt_mixphase = istft_from_mag_phase(deq_mag_dbg, mix_phase_dbg, T).clamp(-1, 1)\n",
        "    #torchaudio.save(path + \"recon_from_gt_tokens_mixphase.wav\", recon_gt_mixphase.unsqueeze(0).cpu(), cfg.sr)\n",
        "    torchaudio.save(path + \"recon_from_gt_tokens_mixphase.wav\", recon_gt_mixphase.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote recon_from_gt_tokens_mixphase.wav\")\n",
        "\n",
        "    # === Ground-truth & recon for SQUARE stem ===\n",
        "    # 1) Clean square\n",
        "    torchaudio.save(path + \"target_square_ground_truth.wav\", square.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote target_square_ground_truth.wav\")\n",
        "    # 2) Quantize→dequantize square then reconstruct with SQUARE PHASE\n",
        "    with torch.no_grad():\n",
        "        sq_mag_dbg, sq_phase_dbg = stft_mag_phase(square.to(device))\n",
        "        sq_log_dbg = to_logmag(sq_mag_dbg)\n",
        "        sq_tok_dbg = quantize_logmag(sq_log_dbg)\n",
        "        deq_sq_mag_dbg = dequantize_to_mag(sq_tok_dbg, sq_mag_dbg.shape).to(sq_phase_dbg.device)\n",
        "        sq_recon_stemphase = istft_from_mag_phase(deq_sq_mag_dbg, sq_phase_dbg, T).clamp(-1, 1)\n",
        "    torchaudio.save(path + \"square_recon_from_gt_tokens.wav\", sq_recon_stemphase.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote square_recon_from_gt_tokens.wav\")\n",
        "    # 3) Same dequantized magnitude but use MIXTURE PHASE\n",
        "    with torch.no_grad():\n",
        "        sq_recon_mixphase = istft_from_mag_phase(deq_sq_mag_dbg, mix_phase_dbg, T).clamp(-1, 1)\n",
        "    torchaudio.save(path + \"square_recon_from_gt_tokens_mixphase.wav\", sq_recon_mixphase.unsqueeze(0).cpu(), cfg.sr)\n",
        "    print(\"Wrote square_recon_from_gt_tokens_mixphase.wav\")"
      ]
    }
  ]
}